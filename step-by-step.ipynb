{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78189bba-7334-4c67-b6f6-650828929e03",
   "metadata": {},
   "source": [
    "# Make things disappear with XMem and FGT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de4537-fd27-4947-807e-e71e35522541",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- [ECCV 2022] XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model: https://github.com/hkchengrex/XMem\n",
    "- [ECCV 2022] Flow-Guided Transformer for Video Inpainting: https://github.com/hitachinsk/fgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8778441f-8efd-49f4-9a0c-3287de21b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "except ImportError:\n",
    "    !pip install torch==1.10.1\n",
    "    !pip install torchvision==0.11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d28076-f695-4642-a478-bc37078f0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('CUDA not available. Please connect to a GPU instance if possible.')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e5080-64d8-4fac-8818-d5045fa5c478",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (a) Load data\n",
    "### Download video from YouTube and split into frames\n",
    "- Source: https://huggingface.co/spaces/YiYiXu/it-happened-one-frame-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc12bb-45c6-4aa1-aba3-e21aaed8f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists as path_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04560a-cf27-44c2-9026-30b4dc99a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path_exists('helper.py'):\n",
    "    !wget https://raw.githubusercontent.com/machinelearnear/make-things-disappear-with-XMem-and-FGT/main/helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6131293-3ead-455b-a0e1-74ff2fbc196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import vid2frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32647ab1-d4d5-4dcd-b2c0-c0f51b65492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_url = 'https://youtu.be/KOnfiFOCwH0' # Trump leaves Argentinean president alone on stage at G20\n",
    "video_name = f'videos/{youtube_url.split(\"/\")[-1]}.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d708433-b71b-421a-a887-ed4e4dda197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path_exists(video_name):\n",
    "    skip_frames, path_frames = vid2frames(youtube_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55670a1-6b26-44e3-afeb-9374d252804c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preview the video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06d8c0-f72a-4eaa-9a1c-56e8559ed538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(open(video_name, 'rb').read()).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd097bfe-93a1-4903-ae78-d6b6d6609130",
   "metadata": {},
   "source": [
    "## (b) Segment first-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2378a5a5-dc80-4839-a8f6-b40b24eaa315",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import transformers\n",
    "except ImportError:\n",
    "    !pip install transformers\n",
    "    import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3cd3141-23e6-4fd9-a0b5-ce19b4ad0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f4bedcb-b094-4609-87aa-d6014465fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MobileViTFeatureExtractor, MobileViTForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "feature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/deeplabv3-mobilevit-small\")\n",
    "model = MobileViTForSemanticSegmentation.from_pretrained(\"apple/deeplabv3-mobilevit-small\")\n",
    "\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# logits are of shape (batch_size, num_labels, height, width)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7946e46-d9ac-4f8c-a90b-5cf3fcac681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = np.array(\n",
    "[\n",
    "    [  0,   0,   0], [192,   0,   0], [  0, 192,   0], [192, 192,   0],\n",
    "    [  0,   0, 192], [192,   0, 192], [  0, 192, 192], [192, 192, 192],\n",
    "    [128,   0,   0], [255,   0,   0], [128, 192,   0], [255, 192,   0],\n",
    "    [128,   0, 192], [255,   0, 192], [128, 192, 192], [255, 192, 192],\n",
    "    [  0, 128,   0], [192, 128,   0], [  0, 255,   0], [192, 255,   0],\n",
    "    [  0, 128, 192]\n",
    "],\n",
    "dtype=np.uint8)\n",
    "\n",
    "labels = [\n",
    "    \"background\",\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24def67f-54c0-4f65-9738-54ae6d8306f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = (inputs[\"pixel_values\"].numpy().squeeze().transpose(1, 2, 0)[..., ::-1] * 255).astype(np.uint8)\n",
    "\n",
    "# Class predictions for each pixel.\n",
    "classes = outputs.logits.argmax(1).squeeze().numpy().astype(np.uint8)\n",
    "\n",
    "# Super slow method but it works... should probably improve this.\n",
    "colored = np.zeros((classes.shape[0], classes.shape[1], 3), dtype=np.uint8)\n",
    "for y in range(classes.shape[0]):\n",
    "    for x in range(classes.shape[1]):\n",
    "        colored[y, x] = palette[classes[y, x]]\n",
    "\n",
    "# Resize predictions to input size (not original size).\n",
    "colored = Image.fromarray(colored)\n",
    "colored = colored.resize((resized.shape[1], resized.shape[0]), resample=Image.Resampling.NEAREST)\n",
    "\n",
    "# Keep everything that is not background.\n",
    "mask = (classes != 0) * 255\n",
    "mask = Image.fromarray(mask.astype(np.uint8)).convert(\"RGB\")\n",
    "mask = mask.resize((resized.shape[1], resized.shape[0]), resample=Image.Resampling.NEAREST)\n",
    "\n",
    "# Blend with the input image.\n",
    "resized = Image.fromarray(resized)\n",
    "highlighted = Image.blend(resized, mask, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93632c3-0b1c-44c9-95c5-b8b661b58200",
   "metadata": {},
   "source": [
    "### Preview first-frame annotation\n",
    "The first frame mask is a PNG with a color palette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff165e-fe53-443d-84de-cc16081a0044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "IPython.display.Image('masks/0.png', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53923cfb-d388-4180-9cfc-ca8bda8b58e8",
   "metadata": {},
   "source": [
    "### Convert the mask to a numpy array\n",
    "Note that the object IDs should be consecutive and start from `1` (`0` represents the background). \n",
    "If they are not, see `inference.data.mask_mapper` and `eval.py` on how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0115d-c9b4-40fe-8d29-c71368a29cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5e2ef-63c8-40de-8513-5acb898c3755",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(Image.open('masks/0.png'))\n",
    "print(np.unique(mask))\n",
    "num_objects = len(np.unique(mask)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d40495-a306-4143-a8c4-1b765ca01e38",
   "metadata": {},
   "source": [
    "## (c) Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model\n",
    "- Source: https://colab.research.google.com/drive/1RXK5QsUo2-CnOiy5AOSjoZggPVHOPh1m?usp=sharing#scrollTo=MWGdN7XCSYSm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59856b-e363-4219-9df6-6085653c14fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get our code and install pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726f5ce-8db1-4acb-83f3-f376d4b5e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path_exists('XMem'):\n",
    "    !git clone https://github.com/hkchengrex/XMem.git\n",
    "    !pip install -r XMem/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0abd7-d62b-4d31-977b-e15335aa9512",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc4a3f-8bbf-4ea8-88da-3681137bf4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path_exists('XMem/saves/XMem.pth'):\n",
    "    !wget -P ./XMem/saves/ https://github.com/hkchengrex/XMem/releases/download/v1.0/XMem.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441ea10-0a7e-40a1-8f18-f3dfaee15ed6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986cc69-1206-4973-b5e5-74edfbb73eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from argparse import ArgumentParser\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from XMem.inference.data.test_datasets import LongTestDataset, DAVISTestDataset, YouTubeVOSTestDataset\n",
    "from XMem.inference.data.mask_mapper import MaskMapper\n",
    "from XMem.model.network import XMem\n",
    "from XMem.inference.inference_core import InferenceCore\n",
    "\n",
    "from XMem.progressbar import progressbar\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# default configuration\n",
    "config = {\n",
    "    'top_k': 30,\n",
    "    'mem_every': 5,\n",
    "    'deep_update_every': -1,\n",
    "    'enable_long_term': True,\n",
    "    'enable_long_term_count_usage': True,\n",
    "    'num_prototypes': 128,\n",
    "    'min_mid_term_frames': 5,\n",
    "    'max_mid_term_frames': 10,\n",
    "    'max_long_term_elements': 10000,\n",
    "}\n",
    "\n",
    "network = XMem(config, './XMem/saves/XMem.pth').eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ff742-63c9-4ea3-b897-76ff1ce22cc1",
   "metadata": {},
   "source": [
    "### Propagate frame-by-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54585f3-fff3-468d-80ba-841254105842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from inference.interact.interactive_utils import image_to_torch, index_numpy_to_one_hot_torch, torch_prob_to_numpy_mask, overlay_davis\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "processor = InferenceCore(network, config=config)\n",
    "processor.set_all_labels(range(1, num_objects+1)) # consecutive labels\n",
    "cap = cv2.VideoCapture(video_name)\n",
    "\n",
    "# You can change these two numbers\n",
    "frames_to_propagate = 200\n",
    "visualize_every = 20\n",
    "\n",
    "current_frame_index = 0\n",
    "\n",
    "with torch.cuda.amp.autocast(enabled=True):\n",
    "    while (cap.isOpened()):\n",
    "    # load frame-by-frame\n",
    "    _, frame = cap.read()\n",
    "    if frame is None or current_frame_index > frames_to_propagate:\n",
    "        break\n",
    "\n",
    "    # convert numpy array to pytorch tensor format\n",
    "    frame_torch, _ = image_to_torch(frame, device=device)\n",
    "    if current_frame_index == 0:\n",
    "        # initialize with the mask\n",
    "        mask_torch = index_numpy_to_one_hot_torch(mask, num_objects+1).to(device)\n",
    "        # the background mask is not fed into the model\n",
    "        prediction = processor.step(frame_torch, mask_torch[1:])\n",
    "    else:\n",
    "        # propagate only\n",
    "        prediction = processor.step(frame_torch)\n",
    "\n",
    "    # argmax, convert to numpy\n",
    "    prediction = torch_prob_to_numpy_mask(prediction)\n",
    "\n",
    "    if current_frame_index % visualize_every == 0:\n",
    "        visualization = overlay_davis(frame, prediction)\n",
    "        display(Image.fromarray(visualization))\n",
    "\n",
    "    current_frame_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a4241-3558-448d-822c-571c1b53e18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4550d-af9f-4718-8047-4a5310133599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c3cdf-b239-42f8-8bad-ca661ec467d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearnear-default:Python",
   "language": "python",
   "name": "conda-env-machinelearnear-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
